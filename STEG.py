# -*- coding: utf-8 -*-
"""Project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z9DPYmU5saitcL9DgQwyQvmcrK8KqtV_

**The Name of the group:**
1- Mohammed - mohammedanssem@gmail.com
2- Shrouq mishal - shrouq.mishal@gmail.com
3- Hind Aljuaid - Hind.a.aljuaid@gmail.com
4- abeer alharbi - abeerhrbb@gmail.com

# **Importing the dataset**

**Mounting the drive**
"""

from google.colab import drive

drive.mount('/content/drive')

cd drive/MyDrive/Project 1

ls

"""**Reading the two training/test datasets**"""

import pandas as pd
import matplotlib.pyplot as plt
client_train=pd.read_csv('client_train.csv')
invoice_train=pd.read_csv('invoice_train.csv')

"""# **EDA**"""

client_train.head()

client_train.shape

invoice_train.head()

invoice_train.shape

"""**Joining the two datasets**"""

merged_df = pd.merge(invoice_train, client_train, on='client_id')

merged_df.head()

merged_df.info()

merged_df['target'].value_counts()
# From the result we discover that the data imbalanced

"""**Solving imbalanced data by under sampling**"""

legit = merged_df[merged_df.target==0]
fraud = merged_df[merged_df.target==1]

print(legit.shape)
print(fraud.shape)

legit_sample = legit.sample(n=21353)

merged_df = pd.concat([legit_sample,fraud],axis=0)

merged_df['target'].value_counts()

merged_df.describe()

merged_df.isnull().sum()

"""------>  No null values , but apparently there is more than one  record per cliend Id"""

# Check for duplicate records
duplicate_records = merged_df.duplicated()

# Get the duplicate rows
duplicate_rows = merged_df[duplicate_records]

# Display the duplicate rows
print(duplicate_rows)

# Remove duplicate records
merged_df = merged_df.drop_duplicates()

# Display the DataFrame without duplicates
print(merged_df)

merged_df.shape

"""-------> 11 Duplicated rows dropped"""

summary_stats = merged_df.describe()
print(summary_stats)

"""**Exploring each feature**

having a look to the unique values of each feature and comparing them whith the expected values based on the readme file
"""

for column in merged_df.columns:
    unique_values = merged_df[column].unique()
    num_unique_values = merged_df[column].nunique()
    print(f" {num_unique_values} Unique values in {column}:")
    print(unique_values)
    print()

for column in merged_dftest.columns:
    unique_values = merged_dftest[column].unique()
    num_unique_values = merged_dftest[column].nunique()
    print(f" {num_unique_values} Unique values in {column}:")
    print(unique_values)
    print()

"""Remarques:


*   there is more than one entry for each client
*   Some wrong entries in counter statue, some entries are string/ and some are integers, and there is more than 5 unique values (we expect to have 5 unique values)
*   Some high values in months number

**Exploring/fixing the issues of counter statue**
"""

merged_df["counter_statue"].value_counts()

merged_df = merged_df[merged_df['counter_statue'] != 'A']
merged_df["counter_statue"].value_counts()

merged_df['counter_statue'] = merged_df['counter_statue'].astype(int)
merged_df["counter_statue"].value_counts()

merged_df = merged_df[merged_df['counter_statue'] <= 5]
merged_df["counter_statue"].value_counts()

"""**Exploring/Fixing the issues for months number**"""

merged_df["months_number"].value_counts()

merged_dftest["months_number"].value_counts()

# To calculate the difference between the first day in the creation_date feature and today
26*12 + 6

creation_date_min = merged_df['creation_date'].min()

# Display the statistics
print(creation_date_min)
from dateutil.relativedelta import relativedelta
import datetime
today = datetime.date.today()

# Format the date in dd/mm/yyyy format
today_formatted = today.strftime("%d/%m/%Y")
datetime_object = datetime.date(1997, 1, 1)
# Calculate the difference in months
months_diff = relativedelta(today, datetime_object)

# Display the difference in months
print("Difference:", months_diff)

merged_df[merged_df["months_number"] > 319]["months_number"].value_counts()

"""--> So from the day the first client joined till now there is only 318/319 month, so months > 318 obviously wrong data

SO all those 1280 Values > 319 will be corrected to eaqual 319

**Dealing with outliers**
"""

# Calculate the value counts and percentages
value_counts = merged_df["tarif_type"].value_counts()
percentages = (value_counts / len(merged_df)) * 100

# Create a DataFrame to store the unique values and their percentages
result_df = pd.DataFrame({'Unique Values': value_counts.index, 'Percentage': percentages})

# Sort the DataFrame by unique values (optional, remove this line if you don't want sorting)
result_df.sort_values(by='Percentage', inplace=True)

# Print the result
print(result_df)

merged_df = merged_df[merged_df["tarif_type"].isin([11, 40, 10,15,45])]

# Calculate the value counts and percentages
value_counts = merged_df["consommation_level_1"].value_counts()
percentages = (value_counts / len(merged_df)) * 100

# Create a DataFrame to store the unique values and their percentages
result_df = pd.DataFrame({'Unique Values': value_counts.index, 'Percentage': percentages})

# Sort the DataFrame by unique values (optional, remove this line if you don't want sorting)
result_df.sort_values(by='Unique Values', inplace=True)

# Print the result
print(result_df)

merged_df = merged_df[merged_df["consommation_level_1"] <= 73995]

merged_df["consommation_level_1"]

merged_dfConsom1outliers = merged_df[merged_df["consommation_level_1"] >= 5000]

# Calculate the value counts and percentages
value_counts = merged_dfConsom1outliers["consommation_level_1"].value_counts()
percentages = (value_counts / len(merged_df)) * 100

# Create a DataFrame to store the unique values and their percentages
result_df = pd.DataFrame({'Unique Values': value_counts.index, 'Percentage': percentages})

# Sort the DataFrame by unique values (optional, remove this line if you don't want sorting)
result_df.sort_values(by='Percentage', inplace=True)

# Print the result
print(result_df)

merged_df = merged_df[merged_df["consommation_level_1"] <= 5000]

# Calculate the value counts and percentages
value_counts = merged_df["consommation_level_2"].value_counts()
percentages = (value_counts / len(merged_df)) * 100

# Create a DataFrame to store the unique values and their percentages
result_df = pd.DataFrame({'Unique Values': value_counts.index, 'Percentage': percentages})

# Sort the DataFrame by unique values (optional, remove this line if you don't want sorting)
result_df.sort_values(by='Percentage', inplace=True)

# Print the result
print(result_df)

merged_dfConsom2outliers = merged_df[merged_df["consommation_level_2"] >= 1800]

# Calculate the value counts and percentages
value_counts = merged_dfConsom2outliers["consommation_level_2"].value_counts()
percentages = (value_counts / len(merged_df)) * 100

# Create a DataFrame to store the unique values and their percentages
result_df = pd.DataFrame({'Unique Values': value_counts.index, 'Percentage': percentages})

# Sort the DataFrame by unique values (optional, remove this line if you don't want sorting)
result_df.sort_values(by='Percentage', inplace=True)

# Print the result
print(result_df)

merged_df = merged_df[merged_df["consommation_level_2"] < 1800]

value_counts = merged_df["consommation_level_3"].value_counts()
percentages = (value_counts / len(merged_df)) * 100

# Create a DataFrame to store the unique values and their percentages
result_df = pd.DataFrame({'Unique Values': value_counts.index, 'Percentage': percentages})

# Sort the DataFrame by unique values (optional, remove this line if you don't want sorting)
result_df.sort_values(by='Percentage', inplace=True)

# Print the result
print(result_df)

merged_dfConso3 = merged_df[merged_df["consommation_level_3"] > 2400]

value_counts = merged_dfConso3["consommation_level_3"].value_counts()
percentages = (value_counts / len(merged_df)) * 100

# Create a DataFrame to store the unique values and their percentages
result_df = pd.DataFrame({'Unique Values': value_counts.index, 'Percentage': percentages})

# Sort the DataFrame by unique values (optional, remove this line if you don't want sorting)
result_df.sort_values(by='Percentage', inplace=True)

# Print the result
print(result_df)

merged_df = merged_df[merged_df["consommation_level_3"] <= 2400]

"""**Exploring relations between features**"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Calculate the correlation matrix
correlation_matrix = merged_df.corr()

# Print the correlation matrix

plt.figure(figsize=(12,10))

sns.heatmap(correlation_matrix, annot=True, cmap=plt.cm.CMRmap_r)
plt.show()

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(merged_df, 0.8)
len(set(corr_features))
corr_features

"""**Transforming features**"""

merged_df.info()

"""From the result we found 4 features are a categorical and in the following cell we transform them into numerical (creation_date, invoice_date, counter_type, client_id)"""

merged_df['creation_date'] = pd.to_datetime(merged_df['creation_date'])
merged_df['Creation_year'] = merged_df['creation_date'].dt.year
merged_df['Creation_month'] = merged_df['creation_date'].dt.month
merged_df['Creation_day'] = merged_df['creation_date'].dt.day

merged_df['invoice_date'] = pd.to_datetime(merged_df['invoice_date'])
merged_df['invoice_year'] = merged_df['invoice_date'].dt.year
merged_df['invoice_month'] = merged_df['invoice_date'].dt.month
merged_df['invoice_day'] = merged_df['invoice_date'].dt.day

merged_df["counter_type"].value_counts()

merged_df["Creation_month"].value_counts()

from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()

# To transform counter_type to numerical data
merged_df=pd.get_dummies(merged_df, columns=['counter_type'])

# To transform client_id to numerical data
merged_df['client_id'] = merged_df["client_id"].str.split('_').str[2]

merged_df['client_id']=merged_df['client_id'].astype(int)

merged_df.info()

merged_df.tail()

"""# **Model Creation**"""

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

# Split data into features (X) and target variable (y)
# we delete old_index because we found its a high correlation with new_index and the same reason to counter_type_GAZ with counter_type_ELEC
X = merged_df.drop(['target','invoice_date','creation_date','old_index','counter_type_GAZ'], axis=1)

y = merged_df["target"]

# Split data into features (X) and target variable (y)
# we split the train data into train and test and we didn't use the test data because it's don't have a target feature
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Init classifier
xgb_cl = xgb.XGBClassifier()

# Fit
xgb_cl.fit(X_train, y_train)

# Predict
preds = xgb_cl.predict(X_test)

# Score
accuracy=accuracy_score(y_test, preds)
precision = precision_score(y_test, preds)
recall = recall_score(y_test, preds)
f1 = f1_score(y_test, preds)
auc = roc_auc_score(y_test, preds)
# Print the model's performance
print('Accuracy',accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 score:", f1)
print("AUC-ROC:", auc)

# Plot the ROC curve
plt.plot(fpr, tpr, label="AUC = {0:.2f}".format(auc))
plt.fill_between(fpr, tpr, color="blue")
plt.xlabel("False positive rate")
plt.ylabel("True positive rate")
plt.legend()
plt.show()